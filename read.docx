# Project Evolution: MNIST Classifier

This document outlines the development process and features of the MNIST Digit Classifier project.

## Initial Goal

The primary objective was to build a functional Convolutional Neural Network (CNN) using PyTorch to classify handwritten digits from the standard MNIST dataset. This aimed to showcase foundational skills in:

*   **Machine Learning:** Understanding classification tasks, model training, and evaluation.
*   **Deep Learning:** Implementing a CNN architecture.
*   **Data Processing:** Loading, transforming, and batching data using `torchvision`.
*   **Python Programming:** Structuring a project with modular code (`data_loader.py`, `model.py`, `train.py`, `inference.py`, `main.py`).

## Core Technologies Used (Initial Phase)

*   **Python:** The core programming language.
*   **PyTorch:** For building and training the neural network.
*   **Torchvision:** For accessing the MNIST dataset and applying standard image transformations.
*   **NumPy:** For numerical operations (often implicitly used by PyTorch).
*   **Matplotlib:** For visualizing data samples, training history, and evaluation results (like the confusion matrix).

## Upscaling the Project: Showcasing Advanced Skills

To demonstrate more advanced ML engineering practices beyond basic model building, the project was enhanced with:

1.  **Experiment Tracking (using MLflow)**
2.  **Hyperparameter Tuning (using Optuna)**

---

## Enhancement 1: Experiment Tracking with MLflow

*   **What is it?** MLflow is an open-source platform for managing the end-to-end machine learning lifecycle. We used its **Tracking** component.
*   **Why use it?**
    *   **Reproducibility:** Logs exactly which code, parameters, and data were used for each training run.
    *   **Comparison:** Provides a UI (`mlflow ui`) to easily compare metrics (like accuracy, loss) across different runs and hyperparameter sets.
    *   **Organization:** Keeps track of all experiments, associated artifacts (models, plots), parameters, and metrics in a structured way (locally in `mlruns` folder or on a remote server).
    *   **MLOps Practice:** Demonstrates understanding of standard tools used in production ML workflows for tracking and managing models.
*   **How it was implemented:**
    1.  Added `mlflow` to `requirements.txt`.
    2.  In `src/train.py`:
        *   Imported `mlflow` and `mlflow.pytorch`.
        *   The `train_model` function now wraps the entire training loop within a `with mlflow.start_run():` block.
        *   **Parameters Logging:** `mlflow.log_param()` is used at the beginning of the run to log hyperparameters like learning rate, momentum, epochs, batch size.
        *   **Metrics Logging:** `mlflow.log_metric()` is used within the training loop (at the end of each epoch) to log training/test loss and accuracy. The `step` argument is used to associate metrics with epochs.
        *   **Model Logging:** `mlflow.pytorch.log_model()` saves the best performing model directly to MLflow artifacts during training, and also logs the final model.
        *   **Artifact Logging:** The training history plot generated by `plot_training_history` is now logged as an image artifact using `mlflow.log_figure()`.
    3.  In `main.py`:
        *   Added command-line arguments for `mlflow_tracking_uri` and `mlflow_experiment_name`.
        *   These arguments are passed down to `train_model`.
        *   Modified the evaluation step (`--skip_training`) to first attempt loading the locally saved model, and if that fails, attempt loading the latest 'best' model logged to MLflow for the specified experiment.
*   **Usage:** Running `python main.py` automatically logs the run. Use `mlflow ui` in the terminal (within the project directory) to view the results.

---

## Enhancement 2: Hyperparameter Tuning with Optuna

*   **What is it?** Optuna is an automatic hyperparameter optimization framework. It efficiently searches through different combinations of hyperparameters to find the set that yields the best performance for a given objective (e.g., maximizing validation accuracy).
*   **Why use it?**
    *   **Efficiency:** Smarter than grid search or random search, often finding good hyperparameters faster.
    *   **Automation:** Avoids the tedious and often inefficient process of manual hyperparameter tuning.
    *   **Better Performance:** Can discover non-intuitive hyperparameter combinations that lead to better model results.
    *   **Showcases Optimization Skills:** Demonstrates knowledge of techniques crucial for maximizing model performance.
*   **How it was implemented:**
    1.  Added `optuna` to `requirements.txt`.
    2.  Created a new script: `tune.py`.
    3.  **Objective Function:** Defined an `objective(trial, args)` function within `tune.py`. This function:
        *   Takes an Optuna `trial` object.
        *   Uses `trial.suggest_float()` (or `suggest_int`, `suggest_categorical`) to get hyperparameter values (e.g., learning rate, momentum) suggested by Optuna for this specific trial.
        *   Creates the model and data loaders.
        *   Calls the *existing* `train_model` function (which now includes MLflow logging) with the suggested hyperparameters. Each call to `train_model` within `objective` becomes a *nested* MLflow run.
        *   Returns the metric that Optuna should optimize (in our case, `best_test_acc` returned by `train_model`).
    4.  **Study Setup:** The main block of `tune.py`:
        *   Creates an Optuna `study` object, specifying the optimization `direction` (`"maximize"` accuracy) and `storage` (a SQLite database `optuna_mnist.db` to save study progress). `load_if_exists=True` allows resuming interrupted studies.
        *   Sets up a *parent* MLflow run to track the overall tuning process.
        *   Calls `study.optimize(objective, n_trials=...)`, which repeatedly calls the `objective` function for the specified number of trials.
        *   Logs the parameters and results of the *best* trial found by Optuna to the parent MLflow run for easy reference.
        *   Connects Optuna trials to MLflow runs using tags (like `optuna_trial_number` and `mlflow.parentRunId`) for better traceability in the MLflow UI.
*   **Usage:** Running `python tune.py --n_trials 20` starts the optimization process for 20 trials. Results can be seen in the MLflow UI (a parent run summarizing the study, with nested runs for each trial) and persisted in the `optuna_mnist.db` file.

---

By adding MLflow and Optuna, the project now demonstrates a more mature ML workflow, covering not just model implementation but also experiment management and automated optimization â€“ key skills for real-world ML applications.